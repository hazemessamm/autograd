{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import autograd\n",
    "from autograd import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR Gate\n",
    "xs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "ys = [0, 1, 1, 1]\n",
    "\n",
    "# Torch OR Gate\n",
    "txs = torch.tensor(xs)\n",
    "tys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogradManualModel:\n",
    "    def __init__(self, nin, layer_dims=[3,4,1], activation='relu'):\n",
    "        self.nin = nin\n",
    "        self.layer_dims = [nin] + layer_dims\n",
    "        self.w = []\n",
    "        for i in range(1, len(self.layer_dims)):\n",
    "            self.w.append([[autograd.Variable(np.random.uniform(-0.1, 0.1)) for i in range(self.layer_dims[i-1])] for _ in range(self.layer_dims[i])])\n",
    "        \n",
    "        self.b = [autograd.Variable(np.random.uniform(-0.1, 0.1)) for i in range(len(layer_dims))]\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = autograd.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = autograd.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f'available activations (relu, sigmoid). Recieved: {activation}')\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return backend.flatten(self.w)\n",
    "    \n",
    "    @property\n",
    "    def biases(self):\n",
    "        return backend.flatten(self.b)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        inputs = x\n",
    "\n",
    "        # Define the graph\n",
    "        current_operations = []\n",
    "        total_operations = []\n",
    "        for i, (layer_weights, layer_biases) in enumerate(zip(self.w, self.b)):\n",
    "            for curr_w in layer_weights:\n",
    "                out = sum([w * inp for w, inp in zip(curr_w, inputs)]) + layer_biases\n",
    "                # we do not want to add sigmoid to the last layer\n",
    "                if i != len(self.w) - 1:\n",
    "                    out = self.activation(out)\n",
    "                current_operations.append(out)\n",
    "                total_operations.append(out)\n",
    "            inputs = current_operations\n",
    "            current_operations = []\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchManualModel:\n",
    "    def __init__(self, nin, layer_dims=[3,4,1], activation='relu'):\n",
    "        self.nin = nin\n",
    "        self.layer_dims = [nin] + layer_dims\n",
    "        self.w = []\n",
    "        for i in range(1, len(self.layer_dims)):\n",
    "            self.w.append([[torch.tensor(np.random.uniform(-0.1, 0.1), requires_grad=True) for i in range(self.layer_dims[i-1])] for _ in range(self.layer_dims[i])])\n",
    "        \n",
    "        self.b = [torch.tensor(np.random.uniform(-0.1, 0.1), requires_grad=True) for i in range(len(layer_dims))]\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = torch.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        else:\n",
    "            raise ValueError(f'available activations (relu, sigmoid). Recieved: {activation}')\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return backend.flatten(self.w)\n",
    "    \n",
    "    @property\n",
    "    def biases(self):\n",
    "        return backend.flatten(self.b)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        inputs = x\n",
    "        current_operations = []\n",
    "        for i, (layer_weights, layer_biases) in enumerate(zip(self.w, self.b)):\n",
    "            for curr_w in layer_weights:\n",
    "                out = sum([w * inp for w, inp in zip(curr_w, inputs)]) + layer_biases\n",
    "                # we do not want to add sigmoid to the last layer\n",
    "                if i != len(self.w) - 1:\n",
    "                    out = self.activation(out)\n",
    "                current_operations.append(out)\n",
    "            inputs = current_operations\n",
    "            current_operations = []\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = AutogradManualModel(2, [5, 5, 1])\n",
    "tml = TorchManualModel(2, [5, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weights to be the same\n",
    "# in pytorch and autograd.\n",
    "\n",
    "for torch_weight, autograd_weight in zip(backend.flatten(tml.w), backend.flatten(aml.w)):\n",
    "    torch_weight.data = torch.tensor(autograd_weight.data)\n",
    "    torch_weight.requires_grad = True\n",
    "\n",
    "for torch_bias, autograd_bias in zip(tml.b, aml.b):\n",
    "    torch_bias.data = torch.tensor(autograd_bias.data)\n",
    "    torch_bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(autograd_model, torch_model, epochs=20):\n",
    "    lr = 0.01\n",
    "    for i in range(1, epochs+1):\n",
    "        outs = []\n",
    "        for x in xs:\n",
    "            out = autograd_model(x)\n",
    "            outs.append(out)\n",
    "\n",
    "        # for clarity and auto-completion\n",
    "        loss: autograd.Node = None\n",
    "        for o, y in zip(outs, ys):\n",
    "            if loss is None:\n",
    "                loss = (y - o)**2\n",
    "            else:\n",
    "                loss += (y - o)**2\n",
    "        \n",
    "        loss = loss / len(ys)\n",
    "\n",
    "        loss_val = loss.forward()\n",
    "        print(f'Epoch {i}: autograd_loss_val={loss_val: .4f}', end='  ')\n",
    "        \n",
    "        loss.backward(autograd_model.w)\n",
    "        loss.backward(autograd_model.b)\n",
    "\n",
    "        # Here we do not need to implement something like `zero_grad()` \n",
    "        # because in autograd the gradients are set to `zero` every `backward` call.\n",
    "        for w in autograd_model.weights:\n",
    "            w._data += -lr * w.gradients\n",
    "\n",
    "            # Here we should reset the graph of the weights\n",
    "            # because it will be re-constructed in every call\n",
    "            # this behaviour can be disabled if we replaced the input\n",
    "            # layer with a placeholder and process the inputs batch by batch\n",
    "            # otherwise if we sent multiple inputs independetly the graph values \n",
    "            # from the past inputs will be overwritten by the new passed inputs\n",
    "            w.outcoming_nodes = []\n",
    "        \n",
    "        for b in autograd_model.biases:\n",
    "            b._data += -lr * b.gradients\n",
    "            b.outcoming_nodes = []\n",
    "\n",
    "        # instead of doing w.outcoming_nodes = [] and b.outcoming_nodes = []\n",
    "        # you can call these functions:\n",
    "        # backend.reset_weights_graph(autograd_model.w)\n",
    "        # backend.reset_weights_graph(autograd_model.b)\n",
    "\n",
    "        torch_outs = []\n",
    "        for x in xs:\n",
    "            out = tml(x)\n",
    "            torch_outs.append(out)\n",
    "\n",
    "        # for clarity and auto-completion\n",
    "        torch_loss: torch.Tensor = None\n",
    "        for o, y in zip(torch_outs, ys):\n",
    "            if torch_loss is None:\n",
    "                torch_loss = (y - o)**2\n",
    "            else:\n",
    "                torch_loss += (y - o)**2\n",
    "        \n",
    "        torch_loss = torch_loss / len(ys)\n",
    "\n",
    "        torch_loss.backward(retain_graph=True)\n",
    "        print(f'torch_loss_val={torch_loss.item(): .4f}')\n",
    "\n",
    "\n",
    "        weights = backend.flatten(torch_model.w)\n",
    "        biases = backend.flatten(torch_model.b)\n",
    "\n",
    "        for w in weights:\n",
    "            w.data += -lr * w.grad\n",
    "            # equivalent to `model.zero_grad(set_to_none=True)` or `optimizer.zero_grad(set_to_none=True)`\n",
    "            w.grad = None\n",
    "        \n",
    "        for b in biases:\n",
    "            b.data += -lr * b.grad\n",
    "            # equivalent to `model.zero_grad(set_to_none=True)` or `optimizer.zero_grad(set_to_none=True)`\n",
    "            b.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: autograd_loss_val= 0.8765  torch_loss_val= 0.8765\n",
      "Epoch 2: autograd_loss_val= 0.8490  torch_loss_val= 0.8490\n",
      "Epoch 3: autograd_loss_val= 0.8226  torch_loss_val= 0.8226\n",
      "Epoch 4: autograd_loss_val= 0.7972  torch_loss_val= 0.7972\n",
      "Epoch 5: autograd_loss_val= 0.7728  torch_loss_val= 0.7728\n",
      "Epoch 6: autograd_loss_val= 0.7494  torch_loss_val= 0.7494\n",
      "Epoch 7: autograd_loss_val= 0.7269  torch_loss_val= 0.7269\n",
      "Epoch 8: autograd_loss_val= 0.7053  torch_loss_val= 0.7053\n",
      "Epoch 9: autograd_loss_val= 0.6846  torch_loss_val= 0.6846\n",
      "Epoch 10: autograd_loss_val= 0.6646  torch_loss_val= 0.6646\n",
      "Epoch 11: autograd_loss_val= 0.6455  torch_loss_val= 0.6455\n",
      "Epoch 12: autograd_loss_val= 0.6271  torch_loss_val= 0.6271\n",
      "Epoch 13: autograd_loss_val= 0.6094  torch_loss_val= 0.6094\n",
      "Epoch 14: autograd_loss_val= 0.5925  torch_loss_val= 0.5925\n",
      "Epoch 15: autograd_loss_val= 0.5762  torch_loss_val= 0.5762\n",
      "Epoch 16: autograd_loss_val= 0.5605  torch_loss_val= 0.5605\n",
      "Epoch 17: autograd_loss_val= 0.5455  torch_loss_val= 0.5455\n",
      "Epoch 18: autograd_loss_val= 0.5310  torch_loss_val= 0.5310\n",
      "Epoch 19: autograd_loss_val= 0.5172  torch_loss_val= 0.5172\n",
      "Epoch 20: autograd_loss_val= 0.5038  torch_loss_val= 0.5038\n"
     ]
    }
   ],
   "source": [
    "train(aml, tml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import autograd\n",
    "from autograd import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = autograd.Variable(-2.)\n",
    "y = torch.tensor(-2., requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.relu(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = autograd.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig.backward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(f'{x.gradients: .4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd\n",
    "x = autograd.Variable(5.)\n",
    "y = autograd.Variable(6.)\n",
    "z = autograd.Variable(7.)\n",
    "\n",
    "a1 = autograd.add(x, y, name=\"first_add\")\n",
    "a2 = autograd.multiply(a1, x, name='first_mul')\n",
    "a3 = autograd.multiply(a2, y, name='second_mul')\n",
    "a4 = autograd.multiply(a3, a2, name='third_mul')\n",
    "a5 = autograd.add(a4, z, name='second_add')\n",
    "a6 = autograd.divide(a5, y, name='first_div')\n",
    "a7 = autograd.multiply(a6, x, name='fourth_mul')\n",
    "a8 = autograd.sin(a7, name='sin_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "visited = set()\n",
    "def fun(r):\n",
    "    def _fun(r):\n",
    "        for i in r.outcoming_nodes:\n",
    "            c = (i, r) \n",
    "            if c in visited:\n",
    "                # l.append(c)\n",
    "                pass\n",
    "            else:\n",
    "                l.append(c)\n",
    "                visited.add(c)\n",
    "            _fun(i)\n",
    "    _fun(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "visited = set()\n",
    "def fun(r):\n",
    "    def _fun(r):\n",
    "        for i in r.outcoming_nodes:\n",
    "            c = (i, r) \n",
    "            if c not in visited:\n",
    "                l.append(c)\n",
    "                visited.add(c)\n",
    "        \n",
    "        for i in r.outcoming_nodes:\n",
    "            fun(i)\n",
    "    _fun(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(first_add, <Variable1>),\n",
       " (first_mul, <Variable1>),\n",
       " (fourth_mul, <Variable1>),\n",
       " (first_mul, first_add),\n",
       " (second_mul, first_mul),\n",
       " (third_mul, first_mul),\n",
       " (third_mul, second_mul),\n",
       " (second_add, third_mul),\n",
       " (first_div, second_add),\n",
       " (fourth_mul, first_div),\n",
       " (sin_op, fourth_mul)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(first_add, <Variable1>): 1,\n",
       " (first_mul, first_add): 1,\n",
       " (second_mul, first_mul): 2,\n",
       " (third_mul, second_mul): 2,\n",
       " (second_add, third_mul): 4,\n",
       " (first_div, second_add): 4,\n",
       " (fourth_mul, first_div): 4,\n",
       " (sin_op, fourth_mul): 5,\n",
       " (third_mul, first_mul): 2,\n",
       " (first_mul, <Variable1>): 1,\n",
       " (fourth_mul, <Variable1>): 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8355/148511205.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SigmoidOperation1>\n",
      "[]\n",
      "<ExpOperation3>\n",
      "[]\n",
      "<MultiplyOperation2>\n",
      "[]\n",
      "<MultiplyOperation2>\n",
      "[]\n",
      "<SigmoidOperation1>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>)]\n",
      "<AddOperation4>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>)]\n",
      "<AddOperation4>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>)]\n",
      "<ExpOperation3>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>)]\n",
      "<MultiplyOperation2>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>)]\n",
      "<MultiplyOperation2>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>)]\n",
      "<SigmoidOperation1>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<DivideOperation5>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<DivideOperation5>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<AddOperation4>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<AddOperation4>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<ExpOperation3>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<MultiplyOperation2>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<MultiplyOperation2>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>)]\n",
      "<SigmoidOperation1>\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>), (<DivideOperation5>, <AddOperation4>), (<SigmoidOperation1>, <DivideOperation5>)]\n",
      "[(<MultiplyOperation2>, <Variable1>), (<ExpOperation3>, <MultiplyOperation2>), (<SigmoidOperation1>, <ExpOperation3>), (<AddOperation4>, <ExpOperation3>), (<SigmoidOperation1>, <AddOperation4>), (<DivideOperation5>, <AddOperation4>), (<SigmoidOperation1>, <DivideOperation5>), (<SigmoidOperation1>, <Variable1>)]\n"
     ]
    }
   ],
   "source": [
    "sig.backward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ExpOperation3>, <AddOperation4>, <DivideOperation5>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig._nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig.incoming_nodes.insert(0, sig._nodes[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ExpOperation3>, <ExpOperation3>, <Variable1>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.get_incoming_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10499358540350662"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1050)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'incoming_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6652/4265496178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincoming_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'incoming_nodes'"
     ]
    }
   ],
   "source": [
    "x.incoming_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6893dbd98c6a930ccbe5d86e8d6cc7e76dfcc706a28f542894af544a9ce068a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
